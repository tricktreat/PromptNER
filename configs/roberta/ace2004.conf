train_path = data/datasets/ace2004/ace2004_train_context@2.json
valid_path = data/datasets/ace2004/ace2004_dev_context@2.json
save_path = data/checkpoint/roberta
save_path_include_iteration = False
init_eval = False
save_optimizer = False
train_log_iter = 1
final_eval = False
train_batch_size = 8
epochs = 50
lr = 2e-05
stage_one_lr_scale = 1.5
lr_warmup = 0.1
weight_decay = 0.01
max_grad_norm = 1.0
match_solver = hungarian
type_loss = celoss
match_warmup_epoch = 0
nil_weight = -1.0
match_boundary_weight = 2.0
match_class_weight = 2.0
loss_boundary_weight = 2.0
loss_class_weight = 2.0
deeply_weight = linear
use_masked_lm = False
repeat_gt_entities = 45
eval_every_epochs = 1
split_epoch = 5
config = configs/all.conf
local_rank = -1
world_size = -1
types_path = data/datasets/ace2004/ace2004_types.json
tokenizer_path = cached_models/roberta-large
lowercase = False
sampling_processes = 4
label = ace2004_train
log_path = data/checkpoint/roberta
store_predictions = False
store_examples = False
example_count = None
debug = False
device_id = 0
model_path = cached_models/roberta-large
model_type = roberta_prompt4ner
cpu = False
eval_batch_size = 16
prop_drop = 0.5
freeze_transformer = False
no_overlapping = False
no_partial_overlapping = True
no_duplicate = True
cls_threshold = 0.0
boundary_threshold = 0.7
lstm_layers = 3
decoder_layers = 3
pool_type = max
prompt_number = 50
prompt_type = soft
prompt_length = 2
prompt_individual_attention = False
sentence_individual_attention = True
last_layer_for_loss = 1
withimage = False
seed = 47
cache_path = None